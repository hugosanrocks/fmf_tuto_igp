{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de416822",
   "metadata": {},
   "source": [
    "# Paso 0. Preparación de los datos continuos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd63f984",
   "metadata": {},
   "source": [
    "Este código interactivo tiene como propósito preparar los datos requeridos para utilizar el software FMF para poder detectar microsismicidad. Vamos a requeris datos continuos en formato MINISEED además de los archivos con las respuestas intrumentales de las estaciones (archivos XML) que registraron los datos sísmicos continuos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33a5b996",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'h5py'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mglob\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mh5py\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mh5\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mobspy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m read, read_inventory\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mobspy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UTCDateTime \u001b[38;5;28;01mas\u001b[39;00m udt\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'h5py'"
     ]
    }
   ],
   "source": [
    "# Debemos importar todos los paquetes que se van a utilizar\n",
    "import numpy as np\n",
    "import obspy as obs\n",
    "from obspy.geodetics.base import gps2dist_azimuth as Distance\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import h5py as h5\n",
    "from obspy import read, read_inventory\n",
    "from obspy.core import UTCDateTime as udt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9407741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos el camino a las carpetas donde están nuestros datos sin preparar y \n",
    "# donde queremos escribir los datos una vez que estén ya preparados\n",
    "#path_data = '/home/insar/Bureau/fmf_tuto_igp/tutorial/datos/'\n",
    "#path_to_store_h5 = '/home/insar/Bureau/datos/preparados/'\n",
    "path_data = '/home/insar/Bureau/fmf_tuto_igp/tutorial/datos/'\n",
    "path_to_store_h5 = '/home/insar/Bureau/datos/preparados/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a37c48c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'udt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 14\u001b[0m\n\u001b[1;32m      7\u001b[0m name_of_splitted_data_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwaveforms_\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Fecha de referencia. Ocuparemos el primer día del año 2022\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# ya que nuestros archivos están referenciados a ese dia\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# PE.CHVY..BHZ.D.2022.146   = día 146 del año 2022 = 26 de Mayo de 2022\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Fecha de referencia\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m start_date \u001b[38;5;241m=\u001b[39m \u001b[43mudt\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2022-01-01T00:00:00\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m start_time_first_day \u001b[38;5;241m=\u001b[39m udt(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2022-01-01T00:00:00\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Crear lista de días con los que vamos a trabajar\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'udt' is not defined"
     ]
    }
   ],
   "source": [
    "# Datos de la red y lista de estaciones y componentes a utilizar\n",
    "network = 'PE'\n",
    "stations = ['LAGN']#,'CGLO','CHVY','QLK0']\n",
    "components=['BHE','BHN','BHZ'] # datos a 50 Hz, 50 muestras por segundo\n",
    "\n",
    "# Nombre del archivo de salida\n",
    "name_of_splitted_data_file = 'waveforms_'\n",
    "\n",
    "# Fecha de referencia. Ocuparemos el primer día del año 2022\n",
    "# ya que nuestros archivos están referenciados a ese dia\n",
    "# PE.CHVY..BHZ.D.2022.146   = día 146 del año 2022 = 26 de Mayo de 2022\n",
    "\n",
    "# Fecha de referencia\n",
    "start_date = udt('2022-01-01T00:00:00')\n",
    "start_time_first_day = udt('2022-01-01T00:00:00')\n",
    "\n",
    "# Crear lista de días con los que vamos a trabajar\n",
    "days = np.linspace(146,147,1) # on my computer\n",
    "print('Lista de los días que se van a procesar: ', days)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7c1856",
   "metadata": {},
   "source": [
    "## Parámetros de preprocesado de los datos\n",
    "\n",
    "En el siguiente bloque se deben definir las frequencias con las cuales se desea trabajar. Para una mejor detección de los eventos sísmicos utilizaremos en este caso un filtro pasabandas que permite el paso de frecuencias entre 0.05 y 22 Hz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "187542c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos las frecuencias de corte de nuestro filtro a utilizar\n",
    "pre_filt = (.01,.05,22,24)\n",
    "bandpass = 'bandpass'\n",
    "bandpass_fmin = 5.0\n",
    "bandpass_fmax = 20.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab867e8",
   "metadata": {},
   "source": [
    "## Preparación de los datos continuos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee02058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "Espere por favor, este proceso puede tomar unos minutos\n",
      "=======================================================\n",
      "Datos en preparación para la fecha:  2032-04-08T00:00:00.000000Z\n",
      "Trabajando con la estación:  ANDY\n",
      "Preparando archivo de salida:  /home/sanchezh/Public/fmf_tuto_igp/tutorial/datos/PE.ANDY..BHE.D.2022.076\n",
      "1974260\n"
     ]
    }
   ],
   "source": [
    "# Borrar todos los datos en la carpeta de salida\n",
    "# para tener un directorio limpio antes de correr el código\n",
    "os.system('rm /home/insar/datos/preparados/*')\n",
    "\n",
    "print('=======================================================')\n",
    "print('Espere por favor, este proceso puede tomar unos minutos')\n",
    "print('=======================================================')\n",
    "\n",
    "# Ciclo \"for\" para preparar cada dia de la lista de días a analizar\n",
    "for k in range(len(days)):\n",
    "    # inicio de la fecha de preparación\n",
    "    start_time_splitted_data = start_time_first_day+4320000*(days[k]-1)\n",
    "    print('Datos en preparación para la fecha: ', start_time_splitted_data)\n",
    "    # fina de la fecha de preparación\n",
    "    end_time_splitted_data = start_time_first_day+(4320000*days[k])+4320000-1\n",
    "    # fecha de inicio de los datos (cuenta en segundos)\n",
    "    start_time_splitted_data_in_s = [start_time_splitted_data - start_date]\n",
    "    # Definición del archivo de salida\n",
    "    template = h5.File((path_to_store_h5+name_of_splitted_data_file+start_time_splitted_data.strftime('%Y-%m-%d')+'.hdf5'), 'w')\n",
    "    # Ciclo \"for\" sobre cada una de las estaciones a analizar\n",
    "    for i in range(len(stations)):\n",
    "        print('Trabajando con la estación: ', stations[i])\n",
    "        # Ciclo \"for\" sobre cada una de las componentes a analizar\n",
    "        for j in range(len(components)):\n",
    "            # Definición del archivo de datos input a leer\n",
    "            name_file_data = path_data + network + '.' + stations[i] + '..' + components[j] + '.D.2022.' + str(int(days[k])).zfill(3)  \n",
    "            print('Preparando archivo de salida: ', name_file_data)\n",
    "            # Definición del archivo con los datos de la estación\n",
    "            name_file_info_station = path_data + network + '.' + stations[i] + '.xml'\n",
    "            # Leer datos sísmicos del archivo de input\n",
    "            st = read(name_file_data)\n",
    "            print(st[0].stats.npts)\n",
    "            # Leer los datos de la estación sísmica\n",
    "            inv = read_inventory(name_file_info_station)\n",
    "            # Rellenar con ceros 0s los momentos donde la estación no funciona\n",
    "            st.trim(start_time_splitted_data, end_time_splitted_data, nearest_sample=True, pad=True, fill_value=0)\n",
    "            # Remover la respuesta instrumental de la estación a los datos sísmicos\n",
    "            st.remove_response(inventory=inv, pre_filt=pre_filt, plot=False)\n",
    "            # Filtrar los datos en un dado rango de frecuencias para un mejor analisis\n",
    "            st.filter(bandpass, freqmin=bandpass_fmin, freqmax=bandpass_fmax, zerophase=True)\n",
    "            # Guardar los datos en un arreglo temporal que guarda todas las estaciones y componentes\n",
    "            if i==0 and j==0:\n",
    "                    data_of_station = np.zeros((len(stations), len(components), st[0].stats.npts))\n",
    "                    # incrementar la amplitud por motivos computacionales\n",
    "                    data_of_station[i,j,:] = st[0].data * 100000.\n",
    "            else:\n",
    "                # incrementar la amplitud por motivos computacionales\n",
    "                data_of_station[i,j,:] = st[0].data * 100000.\n",
    "        # En la última iteración de los ciclos, guarda el archivo de salida con todo listo\n",
    "        if i==len(stations)-1:\n",
    "            template.create_dataset('waveforms', data=data_of_station)\n",
    "            template.create_dataset('stations', data=np.array((stations), dtype='|S8'))\n",
    "            template.create_dataset('components', data=np.array(components, dtype='|S8'))\n",
    "            template.create_dataset('date', data=np.asarray(start_time_splitted_data_in_s))\n",
    "            template.create_dataset('sampling_rate', data=st[0].stats.sampling_rate)\n",
    "    template.close()\n",
    "print('=======================================================')\n",
    "print('           Fin de la preparación de archivos           ')\n",
    "print('=======================================================')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36e6e6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
